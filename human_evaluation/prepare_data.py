"""
File: prepare_data.py
Author: Lei Liu

Description: Prepare data for human evaluation.
"""

import os
import csv
import random
import argparse


# Human evaluation: randomly pick up 100 data samples (i.e. 'context-response pairs') from the test set of a
# 'target language' together with 100 corresponding 'responses' generated by four different settings including
# FS-XLT, FS-XLT_prompt, MTL and MTL_prompt, respectively.
def human_eval_data(dir_test_set, dir_human_eval, path_fs, path_fs_prompt, path_mtl, path_mtl_prompt, lang):
    """
    Prepare data for human evaluation on both few-shot cross-lingual learning (FS-XLT) and multi-task learning (MTL).

    Args:
        dir_test_set: The directory to the test set.
        dir_human_eval: The directory to the 100 'context-response pairs' selected for human evaluation.
        path_fs: The file path to all the responses generated by FS-XLT.
        path_fs_prompt: The file path to all the responses generated by FS-XLT_prompt.
        path_mtl: The file path to all the responses generated by MTL.
        path_mtl_prompt: The file path to all the responses generated by MTL_prompt.
        lang: The language of the test set.
    """
    file_path_test_set = os.path.join(dir_test_set, "%s_test.csv" % lang)
    if not os.path.exists(dir_human_eval):
        os.mkdir(dir_human_eval)
    file_path_human_eval = os.path.join(dir_human_eval, "human_eval_%s.csv" % lang)

    # Get the number of 'context-response pairs' in the test set
    num_test_set = 0
    with open(file_path_test_set, 'r', newline='') as file_test_set:
        csv_reader_test_set = csv.reader(file_test_set)
        csv_header_test_set = next(csv_reader_test_set)
        for row in csv_reader_test_set:
            num_test_set += 1

    # Make sure the number of data examples in the test set is sufficient
    if 100 <= num_test_set:
        # Generate 100 random numbers for both FS-XLT and MTL
        random.seed(2023)
        random_numbers_fs = sorted(random.sample(range(num_test_set), 100))
        random_numbers_mtl = sorted(random.sample(range(num_test_set), 100))
        print(random_numbers_fs)
        print(random_numbers_mtl)

        # Read the *.csv file for test set
        with open(file_path_test_set, 'r', newline='') as file_test_set:
            csv_reader_test_set = csv.reader(file_test_set)
            csv_header_test_set = next(csv_reader_test_set)
            # Read the files that contain all the responses generated by model_fs and model_fs_prompt, respectively
            with open(path_fs, 'r', newline='') as model_fs, \
                    open(path_fs_prompt, 'r', newline='') as model_fs_prompt:
                responses_model_fs = model_fs.readlines()
                responses_model_fs_prompt = model_fs_prompt.readlines()

                # Create a *.csv file to save the selected 100 'context-response pairs' together with the corresponding
                # responses generated by FS-XLT and FS-XLT_prompt
                with open(file_path_human_eval, 'w', newline='') as file_human_eval:
                    csv_writer_human_eval = csv.writer(file_human_eval)
                    csv_header_human_eval = ["Random ID", "Context",
                                             "Response A", "Response B",
                                             "Your Answer? (Enter A, B or Neutral)"]
                    csv_writer_human_eval.writerow(csv_header_human_eval)

                    count_fs = 0
                    for row in csv_reader_test_set:
                        # Pick the row with a numbering that matches the current random number
                        if count_fs == random_numbers_fs[0]:
                            # The random number and 'context'. Note that when it comes to human evaluation, we do not
                            # use the 'response' from each selected 'context-response pair'.
                            line = [count_fs, row[2]]
                            # If the random number is an ODD number, place the response generated by FS-XLT in front of
                            # the one generated by FS-XLT_prompt; otherwise, place the one generated by FS-XLT_prompt
                            # in front of FS-XLT.
                            if count_fs % 2 == 1:
                                line.extend([responses_model_fs[count_fs], responses_model_fs_prompt[count_fs], ""])
                            else:
                                line.extend([responses_model_fs_prompt[count_fs], responses_model_fs[count_fs], ""])
                            csv_writer_human_eval.writerow(line)
                            # Remove current random number
                            random_numbers_fs.remove(count_fs)
                            # Terminate the loop when the very last random number has been processed
                            if len(random_numbers_fs) == 0:
                                break
                        count_fs += 1

        # Read the *.csv file for test set
        with open(file_path_test_set, 'r', newline='') as file_test_set:
            csv_reader_test_set = csv.reader(file_test_set)
            csv_header_test_set = next(csv_reader_test_set)
            # Read the files that contain all the responses generated by MTL and MTL_prompt, respectively
            with open(path_mtl, 'r', newline='') as model_mtl, \
                    open(path_mtl_prompt, 'r', newline='') as model_mtl_prompt:
                responses_model_mtl = model_mtl.readlines()
                responses_model_mtl_prompt = model_mtl_prompt.readlines()
                # Create a *.csv file to save the selected 100 'context-response pairs' together with the corresponding
                # responses generated by MTL and MTL_prompt
                with open(file_path_human_eval, 'a', newline='') as file_human_eval:
                    csv_writer_human_eval = csv.writer(file_human_eval)

                    count_mtl = 0
                    for row in csv_reader_test_set:
                        # Pick the row with a numbering that matches the current random number
                        if count_mtl == random_numbers_mtl[0]:
                            # The random number and 'context'. Note that when it comes to human evaluation, we do not
                            # use the 'response' from each selected 'context-response pair'.
                            line = [count_mtl, row[2]]
                            # If the random number is an ODD number, place the response generated by MTL in front of
                            # the one generated by MTL_prompt; otherwise, place the one generated by MTL_prompt in
                            # front of MTL.
                            if count_mtl % 2 == 1:
                                line.extend([responses_model_mtl[count_mtl], responses_model_mtl_prompt[count_mtl], ""])
                            else:
                                line.extend([responses_model_mtl_prompt[count_mtl], responses_model_mtl[count_mtl], ""])
                            csv_writer_human_eval.writerow(line)
                            # Remove current random number
                            random_numbers_mtl.remove(count_mtl)
                            # Terminate the loop when the very last random number has been processed
                            if len(random_numbers_mtl) == 0:
                                break
                        count_mtl += 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # directory to the original test set of MDIA
    parser.add_argument('--dir_mdia_test_set', type=str, required=True)
    # directory to your experimental results (i.e. generated responses)
    parser.add_argument('--dir_experimental_results', type=str, required=True)
    # directory to save the random examples for human evaluation
    parser.add_argument('--dir_human_evaluation', type=str, required=True)
    hparams = parser.parse_args()

    # Germanic and Romance languages for experimentation
    languages_germanic = ['da', 'de', 'no']
    languages_romance = ['es', 'it', 'pt']

    for language in languages_germanic:
        fs = os.path.join(hparams.dir_experimental_results,
                          "fs germanic/few_shot_%s_10/generated_predictions.txt" % language)

        fs_prompt = os.path.join(hparams.dir_experimental_results,
                                 "fs germanic/few_shot_%s_10_prompt/generated_predictions.txt" % language)

        mtl = os.path.join(hparams.dir_experimental_results,
                           "mtl germanic/multitask_en_%s_10/generated_predictions.txt" % language)

        mtl_prompt = os.path.join(hparams.dir_experimental_results,
                                  "mtl germanic/multitask_en_%s_10_prompt/generated_predictions.txt" % language)

        human_eval_data(dir_test_set=hparams.dir_mdia_test_set,
                        dir_human_eval=hparams.dir_human_evaluation,
                        path_fs=fs,
                        path_fs_prompt=fs_prompt,
                        path_mtl=mtl,
                        path_mtl_prompt=mtl_prompt,
                        lang=language)

    for language in languages_romance:
        fs = os.path.join(hparams.dir_experimental_results,
                          "fs romance/few_shot_%s_10/generated_predictions.txt" % language)

        fs_prompt = os.path.join(hparams.dir_experimental_results,
                                 "fs romance/few_shot_%s_10_prompt/generated_predictions.txt" % language)

        mtl = os.path.join(hparams.dir_experimental_results,
                           "mtl romance/multitask_en_%s_10/generated_predictions.txt" % language)

        mtl_prompt = os.path.join(hparams.dir_experimental_results,
                                  "mtl romance/multitask_en_%s_10_prompt/generated_predictions.txt" % language)

        human_eval_data(dir_test_set=hparams.dir_mdia_test_set,
                        dir_human_eval=hparams.dir_human_evaluation,
                        path_fs=fs,
                        path_fs_prompt=fs_prompt,
                        path_mtl=mtl,
                        path_mtl_prompt=mtl_prompt,
                        lang=language)
